{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import rc_car_env\n",
    "import numpy as np\n",
    "from model_interface import ModelInterface\n",
    "\n",
    "env = gym.make('rc_car_env-v0')\n",
    "state = env.reset(simple=True)\n",
    "mi = ModelInterface()\n",
    "\n",
    "default_action = np.array([1, 1])\n",
    "    \n",
    "THRESHOLD = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/automatic-control/lib/python3.7/site-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state [3.5 3.5 3.5 3.5 3.5 3.5]\n",
      "model action: [-0.8857195  1.       ]\n",
      "expected q for model action: 58.400272\n",
      "expected q for input: 55.477055\n",
      "[1 1]\n",
      "reward: -1.3489091950710663\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.99133975 3.5        0.74133975 0.74133975 0.74133975]\n",
      "model action: [-0.7872433  1.       ]\n",
      "expected q for model action: 52.968037\n",
      "expected q for input: 48.878513\n",
      "[1 1]\n",
      "reward: -0.3198833561701089\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.97503678 3.5        0.75764271 0.75764271 0.75764271]\n",
      "model action: [-0.65649253  1.        ]\n",
      "expected q for model action: 52.59165\n",
      "expected q for input: 49.012257\n",
      "[1 1]\n",
      "reward: -0.29208036058633224\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.95873382 3.5        0.77394567 0.77394567 0.77394567]\n",
      "model action: [-0.57732415  1.        ]\n",
      "expected q for model action: 52.360096\n",
      "expected q for input: 49.146046\n",
      "[1 1]\n",
      "reward: -0.26542452593124755\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.94243086 3.5        0.79024863 0.79024863 0.79024863]\n",
      "model action: [-0.53659165  1.        ]\n",
      "expected q for model action: 52.236637\n",
      "expected q for input: 49.27991\n",
      "[1 1]\n",
      "reward: -0.23984628893932403\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.9261279  3.5        0.80655159 0.80655159 0.80655159]\n",
      "model action: [-0.5561036  1.       ]\n",
      "expected q for model action: 52.282505\n",
      "expected q for input: 49.413746\n",
      "[1 1]\n",
      "reward: -0.21528159928657664\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.90982494 3.5        0.82285456 0.82285456 0.82285456]\n",
      "model action: [-0.42337665  1.        ]\n",
      "expected q for model action: 51.9005\n",
      "expected q for input: 49.547657\n",
      "[1 1]\n",
      "reward: -0.19167138406967732\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.89352197 3.5        0.83915752 0.83915752 0.83915752]\n",
      "model action: [-0.33783793  1.        ]\n",
      "expected q for model action: 51.630558\n",
      "expected q for input: 49.661167\n",
      "[1 1]\n",
      "reward: -0.16896107351929102\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.87721901 3.5        0.85546048 0.85546048 0.85546048]\n",
      "model action: [-0.36084107  1.        ]\n",
      "expected q for model action: 51.587772\n",
      "expected q for input: 49.698368\n",
      "[1 1]\n",
      "reward: -0.16155343938252997\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.86091605 3.5        0.87176344 0.87176344 0.87176344]\n",
      "model action: [-0.3217219  1.       ]\n",
      "expected q for model action: 51.37029\n",
      "expected q for input: 49.7354\n",
      "[1 1]\n",
      "reward: -0.1839740739723319\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.84461309 3.5        0.8880664  0.8880664  0.8880664 ]\n",
      "model action: [-0.28629133  1.        ]\n",
      "expected q for model action: 51.183094\n",
      "expected q for input: 49.77245\n",
      "[1 1]\n",
      "reward: -0.20727728318959526\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.82831013 3.5        0.90436936 0.90436936 0.90436936]\n",
      "model action: [-0.366752  1.      ]\n",
      "expected q for model action: 51.288\n",
      "expected q for input: 49.809467\n",
      "[1 1]\n",
      "reward: -0.2315162263454167\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.81200717 3.5        0.92067233 0.92067233 0.92067233]\n",
      "model action: [-0.26525655  1.        ]\n",
      "expected q for model action: 51.250034\n",
      "expected q for input: 49.846485\n",
      "[1 1]\n",
      "reward: -0.2567484194161893\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.7957042  3.5        0.93697529 0.93697529 0.93697529]\n",
      "model action: [-0.2528027  1.       ]\n",
      "expected q for model action: 51.296528\n",
      "expected q for input: 49.88352\n",
      "[1 1]\n",
      "reward: -0.28303619069176067\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.77940124 3.5        0.95327825 0.95327825 0.95327825]\n",
      "model action: [-0.36407128  0.9139142 ]\n",
      "expected q for model action: 51.39333\n",
      "expected q for input: 49.931217\n",
      "[1 1]\n",
      "reward: -0.31044719483086336\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.76309828 3.5        0.96958121 0.96958121 0.96958121]\n",
      "model action: [-0.27126014  0.8213541 ]\n",
      "expected q for model action: 51.402195\n",
      "expected q for input: 49.982273\n",
      "[1 1]\n",
      "reward: -0.3390549942492591\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.74679532 3.5        0.98588417 0.98588417 0.98588417]\n",
      "model action: [-0.307342   0.8451394]\n",
      "expected q for model action: 51.494007\n",
      "expected q for input: 50.033573\n",
      "[1 1]\n",
      "reward: -0.3689397183596126\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.73049236 3.5        1.00218714 1.00218714 1.00218714]\n",
      "model action: [-0.34836793  0.79143965]\n",
      "expected q for model action: 51.589054\n",
      "expected q for input: 50.084965\n",
      "[1 1]\n",
      "reward: -0.4001888131030651\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.71418939 3.5        1.0184901  1.0184901  1.0184901 ]\n",
      "model action: [-0.35890654  0.64723426]\n",
      "expected q for model action: 51.667225\n",
      "expected q for input: 50.136112\n",
      "[1 1]\n",
      "reward: -0.43289789553731195\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.69788643 3.5        1.03479306 1.03479306 1.03479306]\n",
      "model action: [-0.33816132  0.68753976]\n",
      "expected q for model action: 51.727146\n",
      "expected q for input: 50.187412\n",
      "[1 1]\n",
      "reward: -0.46717173107128773\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.68158347 3.5        1.05109602 1.05109602 1.05109602]\n",
      "model action: [-0.40615335  0.59161645]\n",
      "expected q for model action: 51.83736\n",
      "expected q for input: 50.238667\n",
      "[1 1]\n",
      "reward: -0.5031253543849865\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.66528051 3.5        1.06739898 1.06739898 1.06739898]\n",
      "model action: [-0.4126563  0.4666178]\n",
      "expected q for model action: 52.033253\n",
      "expected q for input: 50.290028\n",
      "[1 1]\n",
      "reward: -0.5408853593010097\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.64897755 3.5        1.08370195 1.08370195 1.08370195]\n",
      "model action: [-0.40143716  0.5176787 ]\n",
      "expected q for model action: 52.14333\n",
      "expected q for input: 50.34128\n",
      "[1 1]\n",
      "reward: -0.5805913880830602\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.63267458 3.5        1.10000491 1.10000491 1.10000491]\n",
      "model action: [-0.3596434   0.52971244]\n",
      "expected q for model action: 52.25028\n",
      "expected q for input: 50.41202\n",
      "[1 1]\n",
      "reward: -0.6223978570851292\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.61637162 3.5        1.11630787 1.11630787 1.11630787]\n",
      "model action: [-0.28467172  0.46791026]\n",
      "expected q for model action: 52.369617\n",
      "expected q for input: 50.502903\n",
      "[1 1]\n",
      "reward: -0.6664759637004192\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.60006866 3.5        1.13261083 1.13261083 1.13261083]\n",
      "model action: [-0.2785239   0.46560922]\n",
      "expected q for model action: 52.509922\n",
      "expected q for input: 50.593708\n",
      "[1 1]\n",
      "reward: -0.7130160296014754\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.5837657  3.5        1.14891379 1.14891379 1.14891379]\n",
      "model action: [-0.26648438  0.41237336]\n",
      "expected q for model action: 52.64684\n",
      "expected q for input: 50.66097\n",
      "[1 1]\n",
      "reward: -0.7622302479020397\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.56746274 3.5        1.16521675 1.16521675 1.16521675]\n",
      "model action: [-0.22140919  0.3647182 ]\n",
      "expected q for model action: 52.7914\n",
      "expected q for input: 50.630527\n",
      "[1 1]\n",
      "reward: -0.8143559178748943\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.55115978 3.5        1.18151972 1.18151972 1.18151972]\n",
      "model action: [-0.30492505  0.46723765]\n",
      "expected q for model action: 52.963535\n",
      "expected q for input: 50.55428\n",
      "[1 1]\n",
      "reward: -0.8696592712539988\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.53485681 3.5        1.19782268 1.19782268 1.19782268]\n",
      "model action: [-0.21964751  0.4813658 ]\n",
      "expected q for model action: 53.05527\n",
      "expected q for input: 50.478123\n",
      "[1 1]\n",
      "reward: -0.9284400203138297\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.51855385 3.5        1.21412564 1.21412564 1.21412564]\n",
      "model action: [-0.25388423  0.45747802]\n",
      "expected q for model action: 53.253742\n",
      "expected q for input: 50.40169\n",
      "[1 1]\n",
      "reward: -0.9910367917271075\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.50225089 3.5        1.2304286  1.2304286  1.2304286 ]\n",
      "model action: [-0.3108609  0.5155963]\n",
      "expected q for model action: 53.428394\n",
      "expected q for input: 50.325428\n",
      "[1 1]\n",
      "reward: -1.0578336542184272\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.48594793 3.5        1.24673156 1.24673156 1.24673156]\n",
      "model action: [-0.37853968  0.47663236]\n",
      "expected q for model action: 53.658894\n",
      "expected q for input: 50.24912\n",
      "[1 1]\n",
      "reward: -1.1292680057992246\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.46964497 3.5        1.26303453 1.26303453 1.26303453]\n",
      "model action: [-0.38061178  0.45518544]\n",
      "expected q for model action: 53.85494\n",
      "expected q for input: 50.19408\n",
      "[1 1]\n",
      "reward: -1.2058401628343367\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.453342   3.5        1.27933749 1.27933749 1.27933749]\n",
      "model action: [-0.34377503  0.5713853 ]\n",
      "expected q for model action: 53.73148\n",
      "expected q for input: 50.1346\n",
      "[1 1]\n",
      "reward: -1.2881250953266996\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.43703904 3.5        1.29564045 1.29564045 1.29564045]\n",
      "model action: [-0.2867065  0.6046696]\n",
      "expected q for model action: 53.53092\n",
      "expected q for input: 50.075382\n",
      "[1 1]\n",
      "reward: -1.3767868905618148\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [0.42073608 0.42073608 0.42073608 1.31194341 1.31194341 1.31194341]\n",
      "model action: [-0.32418627  1.        ]\n",
      "expected q for model action: 36.91425\n",
      "expected q for input: 34.46401\n",
      "[1 1]\n",
      "reward: -1.4725967149856747\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [0.40443312 0.40443312 0.40443312 1.32824637 1.32824637 1.32824637]\n",
      "model action: [-0.4245566  1.       ]\n",
      "expected q for model action: 35.905674\n",
      "expected q for input: 32.951115\n",
      "[1 1]\n",
      "reward: -1.5764553028911183\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [0.38813016 0.38813016 0.38813016 1.34454934 1.34454934 1.34454934]\n",
      "model action: [-0.4445417  1.       ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected q for model action: 34.786625\n",
      "expected q for input: 31.48044\n",
      "[1 1]\n",
      "reward: -1.689421361274955\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [0.37182719 0.37182719 0.37182719 1.3608523  1.3608523  1.3608523 ]\n",
      "model action: [-0.51187545  1.        ]\n",
      "expected q for model action: 32.754032\n",
      "expected q for input: 29.42824\n",
      "[1 1]\n",
      "reward: -1.8127477899143003\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [0.35552423 0.35552423 0.35552423 1.37715526 1.37715526 1.37715526]\n",
      "model action: [-0.4685243  1.       ]\n",
      "expected q for model action: 30.54355\n",
      "expected q for input: 27.361177\n",
      "[1 1]\n",
      "reward: -1.9479283458561885\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [0.33922127 0.33922127 0.33922127 1.39345822 1.39345822 1.39345822]\n",
      "model action: [-0.37433657  1.        ]\n",
      "expected q for model action: 28.28766\n",
      "expected q for input: 25.322115\n",
      "[1 1]\n",
      "reward: -2.096758443421461\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [0.32291831 0.32291831 0.32291831 1.40976118 1.40976118 1.40976118]\n",
      "model action: [-0.41990444  1.        ]\n",
      "expected q for model action: 26.363771\n",
      "expected q for input: 23.373537\n",
      "[1 1]\n",
      "reward: -2.2614153508922534\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [0.30661535 0.30661535 0.30661535 1.42606414 1.42606414 1.42606414]\n",
      "model action: [-0.324812  1.      ]\n",
      "expected q for model action: 24.22272\n",
      "expected q for input: 21.388308\n",
      "[1 1]\n",
      "reward: -2.4445654086482094\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [0.29031239 0.29031239 0.29031239 1.44236711 1.44236711 1.44236711]\n",
      "model action: [-0.30566922  1.        ]\n",
      "expected q for model action: 22.193575\n",
      "expected q for input: 19.403292\n",
      "[1 1]\n",
      "reward: -2.6495095227714334\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [0.27400942 0.27400942 0.27400942 1.45867007 1.45867007 1.45867007]\n",
      "model action: [-0.3349244  1.       ]\n",
      "expected q for model action: 20.23337\n",
      "expected q for input: 20.199343\n",
      "[-0.31194162  1.        ]\n",
      "reward: -2.8803838837322973\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [0.25770646 0.25770646 0.25770646 1.47497303 1.47497303 1.47497303]\n",
      "model action: [-0.31769872  1.        ]\n",
      "expected q for model action: 18.18146\n",
      "expected q for input: 18.172853\n",
      "[-0.31194162  1.        ]\n",
      "reward: -3.6247268862314215\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.25299836 3.5        1.47946314 1.47946314 1.47946314]\n",
      "model action: [-0.3354857   0.90035295]\n",
      "expected q for model action: 52.854984\n",
      "expected q for input: 53.023365\n",
      "[-0.32206476  0.78709   ]\n",
      "reward: -3.6235784230236394\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [0.25307189 0.25307189 0.25307189 1.47918028 1.47918028 1.47918028]\n",
      "model action: [-0.3521281  1.       ]\n",
      "expected q for model action: 16.8602\n",
      "expected q for input: 17.161745\n",
      "[-0.32206476  0.78709   ]\n",
      "reward: -0.45291456869201824\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        3.5        3.5        1.47582237 1.47582237 1.47582237]\n",
      "model action: [-0.52131677  1.        ]\n",
      "expected q for model action: 66.2289\n",
      "expected q for input: 65.93515\n",
      "[-0.4487428  1.       ]\n",
      "reward: -3.6378459654690647\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [0.25889835 0.25889835 3.5        1.47189114 1.47189114 0.25889835]\n",
      "model action: [-0.571368  1.      ]\n",
      "expected q for model action: -9.110624\n",
      "expected q for input: -9.133329\n",
      "[-0.4487428  1.       ]\n",
      "reward: -3.5849043773440075\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.260092   3.5        1.47161096 1.47161096 1.47161096]\n",
      "model action: [-0.42982835  0.79230285]\n",
      "expected q for model action: 53.231953\n",
      "expected q for input: 53.253086\n",
      "[-0.43712518  0.78680354]\n",
      "reward: -3.6330559785655745\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [0.25687494 0.25687494 0.25687494 1.47555236 1.47555236 1.47555236]\n",
      "model action: [-0.41513166  1.        ]\n",
      "expected q for model action: 18.223269\n",
      "expected q for input: 18.595064\n",
      "[-0.43712518  0.78680354]\n",
      "reward: -0.5089451635841916\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5       3.5       3.5       1.4779283 1.4779283 1.4779283]\n",
      "model action: [-0.31440195  1.        ]\n",
      "expected q for model action: 65.60692\n",
      "expected q for input: 65.69057\n",
      "[-0.35761887  1.        ]\n",
      "reward: -3.766852002620135\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [0.25415999 0.25415999 3.5        1.47783428 1.47783428 1.47783428]\n",
      "model action: [-0.28733584  1.        ]\n",
      "expected q for model action: 1.858065\n",
      "expected q for input: 1.9628624\n",
      "[-0.35761887  1.        ]\n",
      "reward: -3.561488208143739\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.25862931 3.5        1.47391415 1.47391415 1.47391415]\n",
      "model action: [-0.3351611  0.9463938]\n",
      "expected q for model action: 52.808445\n",
      "expected q for input: 52.91869\n",
      "[-0.36351    0.9086859]\n",
      "reward: -3.500926993688824\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [0.26274465 0.26274465 3.5        1.46949613 1.46949613 1.46949613]\n",
      "model action: [-0.3063669  1.       ]\n",
      "expected q for model action: 3.9107995\n",
      "expected q for input: 4.0804467\n",
      "[-0.36351    0.9086859]\n",
      "reward: -0.41986703779744244\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5       3.5       3.5       1.4697158 1.4697158 1.4697158]\n",
      "model action: [-0.19633037  1.        ]\n",
      "expected q for model action: 65.28598\n",
      "expected q for input: 65.390076\n",
      "[-0.24452722  1.        ]\n",
      "reward: -3.6015679561147844\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [0.25892618 0.25892618 3.5        1.47350534 1.47350534 1.47350534]\n",
      "model action: [-0.22980948  1.        ]\n",
      "expected q for model action: 3.3845239\n",
      "expected q for input: 3.410952\n",
      "[-0.24452722  1.        ]\n",
      "reward: -0.31453632001873655\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5       3.5       3.5       1.4786734 1.4786734 1.4786734]\n",
      "model action: [-0.31218895  1.        ]\n",
      "expected q for model action: 65.60805\n",
      "expected q for input: 65.369446\n",
      "[-0.20168957  1.        ]\n",
      "reward: -3.58091056745806\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [0.25363612 0.25363612 3.5        1.47865649 1.47865649 1.47865649]\n",
      "model action: [-0.2619929  1.       ]\n",
      "expected q for model action: 1.6143516\n",
      "expected q for input: 1.5066856\n",
      "[-0.20168957  1.        ]\n",
      "reward: -0.2954095841227342\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        3.5        3.5        1.47319511 1.47319511 1.47319511]\n",
      "model action: [-0.16596532  1.        ]\n",
      "expected q for model action: 65.24826\n",
      "expected q for input: 65.270996\n",
      "[-0.1764301  1.       ]\n",
      "reward: -3.403672057030458\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [0.26405714 0.26405714 3.5        1.46714325 1.46714325 1.46714325]\n",
      "model action: [-0.17204398  1.        ]\n",
      "expected q for model action: 3.7990746\n",
      "expected q for input: 3.8067956\n",
      "[-0.1764301  1.       ]\n",
      "reward: -3.3745611838860943\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.26519937 3.5        1.46547278 1.46547278 1.46547278]\n",
      "model action: [-0.35321718  0.9538788 ]\n",
      "expected q for model action: 52.86162\n",
      "expected q for input: 52.626606\n",
      "[-0.25784272  1.        ]\n",
      "reward: -3.4402620448343737\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [0.26065771 0.26065771 3.5        1.46963657 1.46963657 1.46963657]\n",
      "model action: [-0.40869406  1.        ]\n",
      "expected q for model action: 3.8396935\n",
      "expected q for input: 3.5995507\n",
      "[-0.25784272  1.        ]\n",
      "reward: -3.5679287053345803\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.25490924 3.5        1.475315   1.475315   1.475315  ]\n",
      "model action: [-0.3100808  1.       ]\n",
      "expected q for model action: 52.654682\n",
      "expected q for input: 52.701694\n",
      "[-0.29745033  0.959756  ]\n",
      "reward: -3.602114836468525\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [0.25270706 3.5        3.5        1.4774566  1.4774566  1.4774566 ]\n",
      "model action: [-0.27452132  1.        ]\n",
      "expected q for model action: -16.537886\n",
      "expected q for input: -16.833311\n",
      "[-0.29745033  0.959756  ]\n",
      "reward: -3.592948728302228\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.25576759 3.5        1.47434388 1.47434388 1.47434388]\n",
      "model action: [-0.29274493  0.95039386]\n",
      "expected q for model action: 52.71448\n",
      "expected q for input: 52.640553\n",
      "[-0.29864576  1.        ]\n",
      "reward: -3.5161959802250187\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [0.26088907 3.5        3.5        1.46918018 1.46918018 1.46918018]\n",
      "model action: [-0.3259681  1.       ]\n",
      "expected q for model action: -13.534605\n",
      "expected q for input: -13.55551\n",
      "[-0.29864576  1.        ]\n",
      "reward: -3.4649212650441807\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.26319582 3.5        1.46689366 1.46689366 1.46689366]\n",
      "model action: [-0.28444543  0.97061145]\n",
      "expected q for model action: 52.70856\n",
      "expected q for input: 52.865345\n",
      "[-0.3016471  0.8943636]\n",
      "reward: -3.5113128411374577\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [0.26002095 3.5        3.5        1.47009135 1.47009135 1.47009135]\n",
      "model action: [-0.21751112  1.        ]\n",
      "expected q for model action: -13.821836\n",
      "expected q for input: -14.548277\n",
      "[-0.3016471  0.8943636]\n",
      "reward: -3.6315960372889404\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.25533939 3.5        1.474672   1.474672   1.474672  ]\n",
      "model action: [-0.404918   0.8737069]\n",
      "expected q for model action: 53.026035\n",
      "expected q for input: 52.722553\n",
      "[-0.32084993  0.9715967 ]\n",
      "reward: -3.667647840424411\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [0.25301032 3.5        3.5        1.47700452 1.47700452 1.47700452]\n",
      "model action: [-0.44324067  1.        ]\n",
      "expected q for model action: -15.306456\n",
      "expected q for input: -16.358671\n",
      "[-0.32084993  0.9715967 ]\n",
      "reward: -3.60515662675467\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.25538285 3.5        1.47462751 1.47462751 1.47462751]\n",
      "model action: [-0.41214654  0.943115  ]\n",
      "expected q for model action: 52.92139\n",
      "expected q for input: 52.927845\n",
      "[-0.37487465  0.9027169 ]\n",
      "reward: -3.5291008877885273\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [0.26044149 3.5        3.5        1.46956519 1.46956519 1.46956519]\n",
      "model action: [-0.39902318  1.        ]\n",
      "expected q for model action: -13.582228\n",
      "expected q for input: -14.328672\n",
      "[-0.37487465  0.9027169 ]\n",
      "reward: -3.5559180493701774\n",
      "--------------------------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state [3.5        0.2626     3.5        1.46740999 1.46740999 1.46740999]\n",
      "model action: [-0.45267332  0.9590213 ]\n",
      "expected q for model action: 53.002064\n",
      "expected q for input: 53.048405\n",
      "[-0.43644527  0.91553134]\n",
      "reward: -3.584104451897794\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [0.26067058 3.5        3.5        1.46934339 1.46934339 1.46934339]\n",
      "model action: [-0.43825302  1.        ]\n",
      "expected q for model action: -13.498777\n",
      "expected q for input: -14.168058\n",
      "[-0.43644527  0.91553134]\n",
      "reward: -3.6641331502152217\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.25695469 3.5        1.47304836 1.47304836 1.47304836]\n",
      "model action: [-0.37511867  0.97710043]\n",
      "expected q for model action: 52.811954\n",
      "expected q for input: 52.964893\n",
      "[-0.42711344  0.9371985 ]\n",
      "reward: -3.6898149747142743\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [0.25527015 3.5        3.5        1.47478216 1.47478216 1.47478216]\n",
      "model action: [-0.30863148  1.        ]\n",
      "expected q for model action: -14.864988\n",
      "expected q for input: -15.250761\n",
      "[-0.42711344  0.9371985 ]\n",
      "reward: -3.639008692201539\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.25765827 3.5        1.47252983 1.47252983 1.47252983]\n",
      "model action: [-0.16711459  1.        ]\n",
      "expected q for model action: 52.43436\n",
      "expected q for input: 52.57703\n",
      "[-0.2536288  1.       ]\n",
      "reward: -3.5796793895237715\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [0.26165816 0.26165816 3.5        1.46874921 1.46874921 1.46874921]\n",
      "model action: [-0.27894968  1.        ]\n",
      "expected q for model action: 3.7404199\n",
      "expected q for input: 3.6950707\n",
      "[-0.2536288  1.       ]\n",
      "reward: -3.43762377032724\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.26351973 3.5        1.4667923  1.4667923  1.4667923 ]\n",
      "model action: [-0.2713103   0.92748845]\n",
      "expected q for model action: 52.761448\n",
      "expected q for input: 52.598576\n",
      "[-0.21981582  0.97373587]\n",
      "reward: -3.4922262799382873\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [0.25978177 3.5        3.5        1.47044752 1.47044752 1.47044752]\n",
      "model action: [-0.23252533  1.        ]\n",
      "expected q for model action: -13.869581\n",
      "expected q for input: -14.075911\n",
      "[-0.21981582  0.97373587]\n",
      "reward: -3.5773838689820483\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.25382957 3.5        1.47623014 1.47623014 1.47623014]\n",
      "model action: [-0.36379874  0.81081194]\n",
      "expected q for model action: 53.055653\n",
      "expected q for input: 52.80628\n",
      "[-0.29118213  0.8878198 ]\n",
      "reward: -3.6209322395842456\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [0.25105445 3.5        3.5        1.47894564 1.47894564 1.47894564]\n",
      "model action: [-0.30111438  1.        ]\n",
      "expected q for model action: -17.399656\n",
      "expected q for input: -18.673307\n",
      "[-0.29118213  0.8878198 ]\n",
      "reward: -3.661478415135238\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.25326405 3.5        1.476805   1.476805   1.476805  ]\n",
      "model action: [-0.2674474  0.8941851]\n",
      "expected q for model action: 52.7533\n",
      "expected q for input: 52.792942\n",
      "[-0.36197194  0.96313375]\n",
      "reward: -0.39225241782202663\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        3.5        3.5        1.47227211 1.47227211 1.47227211]\n",
      "model action: [-0.2629514  1.       ]\n",
      "expected q for model action: 65.45036\n",
      "expected q for input: 65.693985\n",
      "[-0.36197194  0.96313375]\n",
      "reward: -3.546940523063925\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.26088404 0.26088404 1.46930144 1.46930144 1.46930144]\n",
      "model action: [-0.17853652  1.        ]\n",
      "expected q for model action: 38.414066\n",
      "expected q for input: 38.720676\n",
      "[-0.36197194  0.96313375]\n",
      "reward: -0.39358561843659784\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        3.5        3.5        1.47109514 1.47109514 1.47109514]\n",
      "model action: [-0.2985464  1.       ]\n",
      "expected q for model action: 65.51781\n",
      "expected q for input: 65.685074\n",
      "[-0.36197194  0.96313375]\n",
      "reward: -3.645642722662943\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.25433496 3.5        1.47571455 1.47571455 1.47571455]\n",
      "model action: [-0.23416764  0.9126411 ]\n",
      "expected q for model action: 52.673374\n",
      "expected q for input: 52.799046\n",
      "[-0.36197194  0.96313375]\n",
      "reward: -3.6856969040816607\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [0.25177013 3.5        3.5        1.47824386 1.47824386 1.47824386]\n",
      "model action: [-0.24447863  1.        ]\n",
      "expected q for model action: -17.295897\n",
      "expected q for input: -16.979277\n",
      "[-0.36197194  0.96313375]\n",
      "reward: -3.6506585293715728\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.25401092 3.5        1.47598928 1.47598928 1.47598928]\n",
      "model action: [-0.25890908  0.94480324]\n",
      "expected q for model action: 52.658054\n",
      "expected q for input: 52.4762\n",
      "[-0.2049985  1.       ]\n",
      "reward: -3.5795981141827595\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [0.25868013 3.5        3.5        1.47132799 1.47132799 1.47132799]\n",
      "model action: [-0.23161615  1.        ]\n",
      "expected q for model action: -14.12306\n",
      "expected q for input: -14.143293\n",
      "[-0.2049985  1.       ]\n",
      "reward: -3.437769550673794\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.2618156  3.5        1.46819768 1.46819768 1.46819768]\n",
      "model action: [-0.26935655  0.9399087 ]\n",
      "expected q for model action: 52.72745\n",
      "expected q for input: 52.69846\n",
      "[-0.23383789  0.9223841 ]\n",
      "reward: -3.47739461340677\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [0.2591273  3.5        3.5        1.47097541 1.47097541 1.47097541]\n",
      "model action: [-0.34466705  1.        ]\n",
      "expected q for model action: -13.934294\n",
      "expected q for input: -14.599623\n",
      "[-0.23383789  0.9223841 ]\n",
      "reward: -3.606202722624395\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.25394412 0.25394412 1.47647913 1.47647913 1.47647913]\n",
      "model action: [-0.22352497  1.        ]\n",
      "expected q for model action: 38.10178\n",
      "expected q for input: 38.248173\n",
      "[-0.32175598  1.        ]\n",
      "reward: -0.3437498313982185\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        3.5        3.5        1.48055726 1.48055726 1.48055726]\n",
      "model action: [-0.15579976  1.        ]\n",
      "expected q for model action: 65.28549\n",
      "expected q for input: 65.64374\n",
      "[-0.32175598  1.        ]\n",
      "reward: -3.6531480820485123\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.2515025  0.2515025  0.2515025  1.47927954 1.47927954]\n",
      "model action: [-0.21255867  1.        ]\n",
      "expected q for model action: 35.7829\n",
      "expected q for input: 35.73691\n",
      "[-0.32175598  1.        ]\n",
      "reward: -0.35541556252175377\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        3.5        3.5        1.47411953 1.47411953 1.47411953]\n",
      "model action: [-0.21509698  1.        ]\n",
      "expected q for model action: 65.36182\n",
      "expected q for input: 65.59218\n",
      "[-0.32175598  1.        ]\n",
      "reward: -3.5223666741950144\n",
      "--------------------------------\n",
      "\n",
      "\n",
      "state [3.5        0.26005623 0.26005623 1.4704263  1.4704263  1.4704263 ]\n",
      "model action: [-0.30373064  1.        ]\n",
      "expected q for model action: 38.553303\n",
      "expected q for input: 38.580204\n",
      "[-0.32175598  1.        ]\n",
      "reward: -0.3563858799290445\n",
      "--------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    env.render()\n",
    "    ma = mi.get_action(state)\n",
    "    print(\"state\", state)\n",
    "    print(\"model action:\", ma)\n",
    "    print(\"expected q for model action:\", mi.get_action_q(state, ma))\n",
    "    exp_q = mi.get_action_q(state, default_action)\n",
    "    print(\"expected q for input:\", mi.get_action_q(state, default_action))\n",
    "    state, reward, done, info = env.step(default_action)\n",
    "    if exp_q < THRESHOLD:\n",
    "        # default_action = np.array([-1, -1])\n",
    "        # model can take over because it thinks the \"input\" action sucks\n",
    "        default_action = mi.get_action(state)\n",
    "    print(\"reward:\", reward)\n",
    "    print(\"--------------------------------\\n\\n\")\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automatic-control",
   "language": "python",
   "name": "automatic-control"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
